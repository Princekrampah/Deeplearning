{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "Loss functions are the most important part of machine learning and neural networks, as the name suggests, a loss function is nothing but an error in the predictions made by the neural network. A loss function is a mathematical approach in calculating the loss(error)\n",
    "\n",
    "When a neural net makes a decision we need to evaluate the decision and tell the computer or neural net how well or bad the prediction is. We need to calculate this to improve the neural net. The process by which we feed the error(loss) back into the network so as to make adjustments to the weights and biases of the network is called **backward propagation**.\n",
    "\n",
    "To backward propagation is just a way to tell the net how wrong it has performed in making a prediction during training a neural network. We will look closely into backward propagation later in the text but for now think of it like this. \n",
    "\n",
    "The loss function is used tho calculate values of the **gradient descent**, throught the gradient descent, we can then tell the computer whether to reduce or increase the weights and biases. We will take a closer look at gradient descent later in the text. For those of us familiar with machine learning, we may have already come across gradient descent, this the same concept in neural network, pheww!\n",
    "\n",
    "But how do we tell the network how wrong its wrong? For this we need to calculate the a loss(error), lets take a look at the different ways and methods we can use to calculate the error.\n",
    "\n",
    "## Types Of Loss Functions\n",
    "\n",
    "**1. Mean Squared Error (MSE)**\n",
    "\n",
    "The mean squared error is the most easy to understand loss function, to understand this we will use a simple line graph to explain the concept. For those familiar with linear regression, I bet you already came across this loss function, if this is your first time hearing about this function, well not a problem, we'll go over it :)\n",
    "\n",
    "Lets take a set of data and we want to draw a line graph, just like the one below\n",
    "\n",
    "<img src=\"images/mse_image3.png\"  width=\"400\" height=\"400\" />\n",
    "\n",
    "well i draw this in matplotlib(visualization python library) it automatically calculate the best fit line. But how does the library do this?\n",
    "\n",
    "To calculate the best fit line we need to use math and calculas, aahh calculas (:, I know what you are thinking. I promise to keep it simple and straigth to the point.\n",
    "\n",
    "First lets get familiar with some of the terminologies\n",
    "\n",
    "**1. Residuals:** A residual is the vertical distance between a data point and the regression line. Each data point has one residual. They are positive if they are above the regression line and negative if they are below the regression line. This is just the error that isn't explained by the regression line. The lower the value of the **sum of residuals** the better. To have a good best fit line we need to reduce the residuals as much as possible. In other words, the residual is the difference between the actual y-value value and the predicted y-value for the same point on the X-axis.\n",
    " \n",
    " Mathematically the residual can be calculated with the formular below\n",
    " \n",
    "## \\begin{equation}\n",
    " residaul = y_{true} - y_{predicted} \\\\= y - \\hat{y}\n",
    "\\end{equation}\n",
    " \n",
    " \n",
    " <img src=\"images/mse_image4.png\"  width=\"400\" height=\"400\" />\n",
    "\n",
    "\n",
    "**2. Best fit line:** This is the line with the lowest sum of residuals\n",
    "\n",
    "Now that we have gone over some of the terminologies, let's dive into what MSE is, MSE is **mean of the sum of squared residuals**. Mathematically MSE can be expressed as:\n",
    "\n",
    "## \\begin{equation}\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\n",
    "\\end{equation}\n",
    "\n",
    "**Where:**\\\n",
    "$ y $ - true value of y\\\n",
    "$ \\hat{y} $ - the predicted y value\n",
    "\n",
    "The best fit line is the line having the lowest MSE, this is a loss functions since it helps the algorithm to calculate the best values to fit the best fit line(y-intercept and slope). Let's write a simple python programm to calculate the MSE.\n",
    "\n",
    "```python\n",
    "    import numpy as np\n",
    "\n",
    "    def MSE(y_true, y_pred):\n",
    "        return np.mean((y_true-y_pred)**2)\n",
    "\n",
    "    y_true = np.array([12, 10, 13, 14])\n",
    "    y_pred = np.array([10, 15, 13, 15])\n",
    "\n",
    "    MSE(y_true, y_pred)\n",
    "    \n",
    "    >>> 7.5\n",
    "```\n",
    "\n",
    "**2. Mean Absolute Error**\n",
    "\n",
    "This is very similar to the MSE expect it takes the absolute value of the residuals instead. This is just the average of the absolute value of the errors.\n",
    "\n",
    "Mathematically this can be calulated as follow:\n",
    "\n",
    "## \\begin{equation}\n",
    "MAE = \\frac{1}{n}\\sum_{i=1}^{n}(abs(y_i - \\hat{y_i})\\\\= \\frac{1}{n}\\sum_{i=1}^{n} |(y_i - \\hat{y_i}|\n",
    "\\end{equation}\n",
    "\n",
    "Now, let's take a look at the Python code for MAE\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def MAE(y_true, y_pred):\n",
    "    return np.mean(abs(y_true-y_pred))\n",
    "\n",
    "y_true = np.array([12, 10, 13, 14])\n",
    "y_pred = np.array([10, 15, 13, 15])\n",
    "\n",
    "MAE(y_true, y_pred)\n",
    "\n",
    ">>> 2\n",
    "```\n",
    "\n",
    "**3. Log loss or Binary cross entropy** \n",
    "\n",
    "This loss function measures the performance of a classification model that outputs a probability between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .027 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. This loss function is suited for binary classification cases where the output is either 0 or 1.\n",
    "\n",
    "Example this loss function can be used in a model that takes the age, wealth and location of an individual and predict whether or not he or she will buy a house.(0 or 1 output).\n",
    "\n",
    "the log loss can be mathematically expressed as:\n",
    "\n",
    "## \\begin{equation}\n",
    "logloss = -\\frac{1}{n}\\sum_{i=1}^{n}\\big(ylog{(\\hat{y})} + (1-y)log{(1 - \\hat{y})}\\big)\n",
    "\\end{equation}\n",
    "\n",
    "Looking at this formula, something you might have noticed is that the $log(0)$ is always undefined, a quetion you might ask yourself if why $ log(0) = 0 $ ? Well to understand this we need to remind ourselfs of basic laws of logarith.\n",
    "\n",
    "### \\begin{equation}\n",
    "log_b{y} = x\\\\\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Expressing the log in exponential form we get\n",
    "\n",
    "### \\begin{equation}\n",
    "b^x = y\n",
    "\\end{equation}\n",
    "\n",
    "This law is only valid if $ y > 0 $, we can't find a number $b$ raised to power of $x$ which will result to zero.\n",
    "hense $ log_b{0} $ **does not exist**. When we create a log loss alogrithm we must make sure wee never get to $ log(0) $. To do this we are going to replace all zero value with a number that is very close to zero we will call this number **epsilon = 1e-15**. This is just fifteen floating zeros followed by one(0.0000000000000001)\n",
    "\n",
    "To further counter this issue of getting a $ log{0} $, we must replace all 1 values with a number that is very close to 1, this is to counter the scenerios where we will get $ log{0} $ in this $  (1-y)log{(1 - \\hat{y})} $  part of our equation, this will occur in cases where the y_predicted is a one($ \\hat{y} = 1 $). This is the way around the $log0$ problem.\n",
    "\n",
    "To replace the 1's we will substract **epsilon** value from 1(1 - 1e-15) == 0.999999...\n",
    "\n",
    "Enough of the talking let's get into the code.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def log_loss(y_true, y_predict):\n",
    "    epsilon = 1e-15\n",
    "    n = len(y_true)\n",
    "    \n",
    "    y_predicted_new = [max(i, epsilon) for i in y_predict]\n",
    "    # this section of the code replaces all zero value with epsilon since, the max() function \n",
    "    # will return the biggest value amongs the two, in this case it's the epsilon.\n",
    "    \n",
    "    y_predicted_new = [min(i, 1-epsilon) for i in y_predicted_new]\n",
    "    # this section of the code will replace all values of 1 with 0.999...\n",
    "    \n",
    "    \n",
    "    y_predicted_new = np.array(y_predicted_new)\n",
    "    # convert this into a numpy array, it's more fast and efficient.     \n",
    "    \n",
    "    print(y_predicted_new)\n",
    "    # let's see what's going on.\n",
    "\n",
    "    return -np.mean(y_true*np.log(y_predicted_new) + (1-y_true)*(np.log(1 - y_predicted_new)))   \n",
    "\n",
    "y_pred = np.array([1,1, 0, 0, 1])\n",
    "y_true = np.array([1, 1, 1, 0, 0])\n",
    "\n",
    "log_loss(y_true, y_pred)\n",
    "\n",
    ">>> 13.815670477450311\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def MSE(y_true, y_pred):\n",
    "    return np.mean((y_true-y_pred)**2)\n",
    "\n",
    "y_true = np.array([12, 10, 13, 14])\n",
    "y_pred = np.array([10, 15, 13, 15])\n",
    "\n",
    "MSE(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MAE(y_true, y_pred):\n",
    "    return np.mean(abs(y_true-y_pred))\n",
    "\n",
    "MAE(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f6e7c0610b57>:1: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.e+00 1.e+00 1.e-15 1.e-15 1.e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.815670477450311"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def log_loss(y_true, y_predict):\n",
    "    epsilon = 1e-15\n",
    "    n = len(y_true)\n",
    "    \n",
    "    y_predicted_new = [max(i, epsilon) for i in y_predict]\n",
    "    # this section of the code replaces all zero value with epsilon since, the max() function \n",
    "    # will return the biggest value amongs the two, in this case it's the epsilon.\n",
    "    \n",
    "    y_predicted_new = [min(i, 1-epsilon) for i in y_predicted_new]\n",
    "    # this section of the code will replace all values of 1 with 0.999...\n",
    "    \n",
    "    \n",
    "    y_predicted_new = np.array(y_predicted_new)\n",
    "    # convert this into a numpy array, it's more fast and efficient.     \n",
    "    \n",
    "    print(y_predicted_new)\n",
    "    # let's see what's going on.\n",
    "\n",
    "    return -np.mean(y_true*np.log(y_predicted_new) + (1-y_true)*(np.log(1 - y_predicted_new)))   \n",
    "\n",
    "y_pred = np.array([1,1, 0, 0, 1])\n",
    "y_true = np.array([1, 1, 1, 0, 0])\n",
    "\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
