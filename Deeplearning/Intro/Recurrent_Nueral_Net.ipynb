{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO RECURRENT NEURAL NETWORKS\n",
    "\n",
    "Apart from the valina feed forward neural network and the convolutional neural network, recurrent neural network(RNN) is another type of neural network.\n",
    "\n",
    "Its used to predicting data that occurs in series, example auto completion has words that occur in series, RNN is used to predict such data, example \"come\" appears before \"here\" in most cases. RNN unlike the tradiitional neural network uses the output of the previsious input to predict the current input. ie is the first input was \"come\" the using this the RNN can predict the next word that is likely to follow \"here\".\n",
    "\n",
    "Try to say the alphabet from A to Z, it quite easy right, now try to say it from Z to A, mmmmh difficult right!. One thing you can notice is that the order in which we pronounce the alphabet helps us to remember what letters follows in the sequence right, well this is what the RNN is built for, predicting the next data in the series when given an input from the series.\n",
    "\n",
    "## What's special about RNN\n",
    "\n",
    "This can not be done by a traditional feed forward neural net because each output prediction is independent of the previous input, they don't memories the previous output(no future scope), each prediction is dependent only on the current input and nothing else. While RNN on the other hand the prediction is dependent on the previous input as well as the current input, it has future scope.\n",
    "\n",
    "While RNNs learns in a similar way to a feed forward neural networks during training, in addition, they remember things learnt from prior input(s) while generating output(s). Refer to the alphabet example I gave above, you need to know a certain letter in yuor head before you can predict another one. ie if you store A in your head then you can know the next is B but without a knowledge of A in memory you can't predict B.\n",
    "\n",
    "You canâ€™t simply predict an arbitrary value for the stock price depending on the time. You must first have knowledge and undestand the trend of the stock, in other words you need to know how the stock price has been moving over time in the past to predict the future stock prices. ie in Januery the prices where good so this maybe the case in February as well.\n",
    "\n",
    "\n",
    "Another example just for clarity, \".... is the time?\" in this example the RNN can't make predictions right since it does not know the previous word in the sentence. \"Hussien .... is the time?\" here in the second sentence the RNN has knowledge on the previous data and hence can make predictions on what the missing word is.\n",
    "\n",
    "\n",
    "![RNN1](./images/RNN1.png)\n",
    "\n",
    "\n",
    "## Applications of RNN\n",
    "\n",
    "1. Image captioning\n",
    "2. Natural Language Processeing(NLP)\n",
    "3. Stock Prediction\n",
    "\n",
    "## How The RNN Works In Depth\n",
    "\n",
    "\n",
    "As explained earlier, the RNN training is similar to the traditional feed forward nueral network except that, the previous input is also used in predicting the output of the next input. To explain this more, lets take the example of a simple RNN that predicts the next word in a sentence.\n",
    "\n",
    "The way we can train the model is to input one word at a time, basing on the word we input first in the series we can predict the next word. To understand the logic and math behind use the figure as a reference for the coming explanations:\n",
    "\n",
    "## Forward Propagation In RNN\n",
    "\n",
    "![RNN2](./images/RNN2.png)\n",
    "\n",
    "**Where:**\n",
    "\n",
    "\\begin{aligned}\n",
    "&x_n \\text{: Inputs}\\\\\n",
    "&y_n \\text{: outputs}\\\\\n",
    "&h_n \\text{: The communication from one time step to another(activation from previous input)}\\\\\n",
    "&w_i, w_y, w_r \\text{: Shared parameters accross the whole RNN}\\\\\n",
    "&g_{h} \\text{: The activation function}\\\\\n",
    "&g_{y} \\text{: The activation function}\n",
    "\\end{aligned}\n",
    "\n",
    "The word in a sentence are feed to the RNN one at a time, and the activation from  the previouse layer is included to make predictions(for the first word, there is no activation of the previous layer so a random value is assigned mostly a zero).\n",
    "\n",
    "The activation function could be sigmoid or a softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
