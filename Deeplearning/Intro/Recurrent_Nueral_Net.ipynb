{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO RECURRENT NEURAL NETWORKS\n",
    "\n",
    "Apart from the valina feed forward neural network and the convolutional neural network, recurrent neural network(RNN) is another type of neural network.\n",
    "\n",
    "Its used to predicting data that occurs in series, example auto completion has words that occur in series, RNN is used to predict such data, example \"come\" appears before \"here\" in most cases. RNN unlike the tradiitional neural network uses the output of the previsious input to predict the current input. ie is the first input was \"come\" the using this the RNN can predict the next word that is likely to follow \"here\".\n",
    "\n",
    "Try to say the alphabet from A to Z, it quite easy right, now try to say it from Z to A, mmmmh difficult right!. One thing you can notice is that the order in which we pronounce the alphabet helps us to remember what letters follows in the sequence right, well this is what the RNN is built for, predicting the next data in the series when given an input from the series.\n",
    "\n",
    "## What's special about RNN\n",
    "\n",
    "This can not be done by a traditional feed forward neural net because each output prediction is independent of the previous input, they don't memories the previous output(no future scope), each prediction is dependent only on the current input and nothing else. While RNN on the other hand the prediction is dependent on the previous input as well as the current input, it has future scope.\n",
    "\n",
    "While RNNs learns in a similar way to a feed forward neural networks during training, in addition, they remember things learnt from prior input(s) while generating output(s). Refer to the alphabet example I gave above, you need to know a certain letter in yuor head before you can predict another one. ie if you store A in your head then you can know the next is B but without a knowledge of A in memory you can't predict B.\n",
    "\n",
    "You canâ€™t simply predict an arbitrary value for the stock price depending on the time. You must first have knowledge and undestand the trend of the stock, in other words you need to know how the stock price has been moving over time in the past to predict the future stock prices. ie in Januery the prices where good so this maybe the case in February as well.\n",
    "\n",
    "\n",
    "Another example just for clarity, \".... is the time?\" in this example the RNN can't make predictions right since it does not know the previous word in the sentence. \"Hussien .... is the time?\" here in the second sentence the RNN has knowledge on the previous data and hence can make predictions on what the missing word is.\n",
    "\n",
    "\n",
    "![RNN1](./images/RNN1.png)\n",
    "\n",
    "\n",
    "## Applications of RNN\n",
    "\n",
    "1. Image captioning\n",
    "2. Natural Language Processeing(NLP)\n",
    "3. Stock Prediction\n",
    "\n",
    "## How The RNN Works In Depth\n",
    "\n",
    "\n",
    "As explained earlier, the RNN training is similar to the traditional feed forward nueral network except that, the previous input is also used in predicting the output of the next input. To explain this more, lets take the example of a simple RNN that predicts the next word in a sentence.\n",
    "\n",
    "The way we can train the model is to input one word at a time, basing on the word we input first in the series we can predict the next word. To understand the logic and math behind use the figure as a reference for the coming explanations:\n",
    "\n",
    "## Forward Propagation In RNN\n",
    "\n",
    "![RNN2](./images/RNN2.png)\n",
    "\n",
    "**Where:**\n",
    "\n",
    "\\begin{aligned}\n",
    "&x_n \\text{: Inputs}\\\\\n",
    "&y_n \\text{: outputs}\\\\\n",
    "&h_n \\text{: The communication from one time step to another(activation from previous input)}\\\\\n",
    "&w_i, w_y, w_r \\text{: Shared parameters accross the whole RNN}\\\\\n",
    "&g_{h} \\text{: The activation function}\\\\\n",
    "&g_{y} \\text{: The activation function}\n",
    "\\end{aligned}\n",
    "\n",
    "The word in a sentence are feed to the RNN one at a time, and the activation from  the previouse layer is included to make predictions(for the first word, there is no activation of the previous layer so a random value is assigned mostly a zero).\n",
    "\n",
    "The activation function could be sigmoid or a softmax\n",
    "\n",
    "## Backward Propagation\n",
    "\n",
    "Just like the feed forward neural network, the RNN needs to calculate the loss and back propagate it to the network. When the loss is calculated the a loss function called the crossentropy loss(standard logistic regression loss) is used.\n",
    "\n",
    "\\begin{aligned}\n",
    "&L(\\hat{y}^{(t)}, y^{(t)}) = -y^{(t)} log \\hat{y}^{(t)} - (1 - y^{(t)}) log(1 - y^{(t)})\\\\\n",
    "&L(\\hat{y}. y) = \\sum_{t = 1}^{T_y} L^{(t)}(\\hat{y}^{(t)}, y^{(t)})\n",
    "\\end{aligned}\n",
    "\n",
    "## Types of RNN\n",
    "\n",
    "### One to One RNN\n",
    "\n",
    "This is the simplist type of RNN in which there is a single output for every single input $$ {T_x = T_y = 1} $$\n",
    "\n",
    "### One to Many\n",
    "\n",
    "This is the RNN architecture that outputs multiple values for a single input value. Its commonly used in music generation where one note can be used to generate many notes(music piece) $$ T_x = 1, T_y>1 $$\n",
    "\n",
    "### Many to One\n",
    "\n",
    "This RNN architecture is used in areas like sentiment analysis in which one output(positive or negative) is produced given multiple inputs(sentence). $$ T_x > 1, T_y = 1 $$\n",
    "\n",
    "Take for example The Twitter sentiment analysis model. In that model, a text input (words as multiple inputs) gives its fixed sentiment (single output). Another example could be movie ratings model that takes review texts as input to provide a rating to a movie that may range from 1 to 5.\n",
    "\n",
    "### Many to Many\n",
    "\n",
    "This is the type of RNN architecture that outputs multiple outputs given multiple inputs. There are two types o f RNN:\n",
    "\n",
    "1. $$ T_x = T_y $$\n",
    "\n",
    "This is the type in which the total number of inputs is same as the total number of output(input size = output size).\n",
    "\n",
    "2. $$ T_x != T_y $$\n",
    "\n",
    "The output size is not equal to the input size. This is commonly used in machine Translation in which the total number of ones in a sentence in one lanuguage is not the same as in another language\n",
    "\n",
    "Example, \"I love you\" in English has three words while in Spanish its \"le amos\"\n",
    "\n",
    "![RNN3](./images/RNN3.png)\n",
    "\n",
    "### More Reading\n",
    "\n",
    "[For More reading](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradient\n",
    "\n",
    "The vanishing and exploding gradient phenomena are often encountered in the context of RNNs. The reason why they happen is that it is difficult to capture long term dependencies because of multiplicative gradient that can be exponentially decreasing/increasing with respect to the number of layers.[source](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)\n",
    "\n",
    "When we talk of the gradient, we mean the gradient of the loss with respect to the weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
