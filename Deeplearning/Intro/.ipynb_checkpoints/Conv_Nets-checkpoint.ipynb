{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO CONVOLUTION NUERAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why convolution Neural Networks?\n",
    "\n",
    "Convolution is needed to reduce the amount of computation we need to do when doing image recognition.\n",
    "\n",
    "Take a 1000 X 1000 pixel image the total number of pixels is 1,000,000 * 3, since we have RGB channel. So the input has 3,000,000 dimension. Now lets imagine you have a NN with 1000 neurons inside the first layer, then the total number of weights will be 3,000,000 * 1000 = 3 billion weights and biases.\n",
    "\n",
    "This is just too much computation to do to solve this issue we use convolution neural networks to reduce the dimensions hence less calculations.\n",
    "\n",
    "## How convolution layer works\n",
    "\n",
    "### 2D convolutions\n",
    "\n",
    "In 2D convolution we simply have the kernel or the filter also called feature map, we \"slide\" the kernel over the input matrix of an image and perform an elment wise operation, multipling each element of the kernel and the input matrix of the image, we later sum up the values into a simple ouput pixel.\n",
    "\n",
    "We repeat this process, sliding the kernel from one position to another performing an element wise product and summing the result to a single output pixel. At the end of sliding the kernel all over the image, we will then get another 2D image with some set of featers the kernel detected. These features are as a result of the summation(weighted sum) after element wise product.of the input features located roughly in the same location of the output pixel on the input layer.\n",
    "\n",
    "The size of the kernel determines how many features get combined into a single output feature,hence the size of the kernel determines the exact location of features on the ouput layer.\n",
    "\n",
    "![convGIF](./images/convGIF.webp)\n",
    "\n",
    "From the above image we have a 5X5 image that is 5 x 5 = 25 this get convoled into a 3X3 = 9 output layer(feature). So by convolution we got a 3X3 output feature from a 5X5 input feature.\n",
    "\n",
    "Imagine we used a standard fully connected NN with 10 neurons for the first layer, we would 5x5x3x10 = 750 weights to calculate and 10 biases making a total of 760 parameters to calculate in only the first layer. That is too much computation just for a 5X5 pixel image.\n",
    "\n",
    "Imagine the case when the image would be 1000X1000 and neurons being 1000 that is about 3 billion weigths to calculate, just too much.\n",
    "\n",
    "In the case of convolution the parameters would be 3x3x3x10 = 270 weights and 10 biases making 280 parameter to calculate, this is almost three times less than using a standard fully connected layer.\n",
    "\n",
    "Hope this gave you an idea of why convolution is used.\n",
    "\n",
    "### Padding\n",
    "\n",
    "In some cases when sliding the kernel over the input matrix some pixels around the edge get cut off since some(the kernel exceeds the edge where the input matrix pixels are, along the edge) Example thing of a 5X5 input matrix and a 5X5 kernel. To solve this we edd to add some pixels to the input matrix along the age to avoid the kernel running over, this technique is what we call padding. Its abit of a headache to grasp but take a look at the image below.\n",
    "\n",
    "![padding](./images/padding.gif)\n",
    "\n",
    "Adding these fake extra pixels to the edge solve the problem, this pixels usaully have the value of 0. Note that we padd all around the image not just a single or a couple of sides.\n",
    "\n",
    "![padding1](./images/padding2.gif)\n",
    "\n",
    "#### Why padding\n",
    "\n",
    "We do padding to deal with\n",
    "\n",
    "- Shrinking output problem, where the output shrinks when passed througth each filter and we loose alot of data\n",
    "- Throwing data along the edges\n",
    "\n",
    "#### How padding preserves the original image\n",
    "\n",
    "Take an image of 6x6 applying a 3x3 filter will cause it to shrink to 4x4, to preserve the original size we can use padding to get 8x8 image and the output size will be 6x6.\n",
    "\n",
    "#### Valid and Same convolutions\n",
    "\n",
    "##### 1. Valid\n",
    "\n",
    "This is the type of convolution in which we do not apply any kind of padding to the original image. This can result in data loss around the edges in case the filter does not perfectly fit the original image\n",
    "\n",
    "##### 2. Same convolution\n",
    "\n",
    "This type of convolution involves applying filters so that the input size is *same* as the ouput size.\n",
    "\n",
    "### Striding\n",
    "\n",
    "Def: How many units of data the filter slides between each operation\n",
    "\n",
    "When sliding the kernel over the input matrix we can decide how much we want to skip, jump or hop from left to right while sliding. This basically means how many columns we want to skip when sliding the kernel.\n",
    "\n",
    "In a standard convolution layer the striding is 1, basically we include every slide or column of the input matrix.\n",
    "stride of 2 means skipping two columns of the input matrix after every convolution operation. The striding we choose to use affects the final size of the output matrix. Striding of 2 downsizes the output by roughly a factor of 2, striding of 3 downsizes the output by roughly a factor of 3.....\n",
    "\n",
    "![striding](./images/striding.gif)\n",
    "\n",
    "![Strinding2](./images/striding2.gif)\n",
    "\n",
    "## Calculating Input And Output Sizes\n",
    "\n",
    "## \\begin{equation}\n",
    "output\\_size = \\left[ \\frac{n + 2p - f}{s} + 1 \\right] * \\left[ \\frac{n + 2p -f}{s} + 1 \\right]\\\\\n",
    "padding\\_to\\_use(p) = \\frac{f - 1}{2}\n",
    "\\end{equation}\n",
    "\n",
    "**Where**:\n",
    "\n",
    "- n = input size\n",
    "\n",
    "- p = padding\n",
    "\n",
    "- f = filter size\n",
    "\n",
    "- s = strides \n",
    "\n",
    "## Multiple channels and Dimenstion\n",
    "\n",
    "A normal colored image has multiple layer of color called channels. There are three main color channels Red, Green and Blue channels \"The RGB\" channels. Increase in the number of channels increase the dimention of the input layer matrix in a fully connected layer. Example taking a 5X5 pixel with a single color channel image the dimensions will be 6X6 = 36, if its a normal RGB image then the deminsions will be 6x6x3 = 108.\n",
    "\n",
    "![RGB](./images/RGB.png)\n",
    "\n",
    "### Difference between a filter and a kernel\n",
    "\n",
    "Earlier on in the text I mentioned that filters and kernels are the same thing and used then interchangeably, well there is a little differnce between the two. Now that we went over channels and the RGB concept its time we understand the difference. A filter is a collection of kernels with each kernel being unique and deals with a single channel of the input layer. There is a kernel for every channel in the input layer, a collection of this channels make a filter.\n",
    "\n",
    "Each filter produces one and only one output channel, how this works is while sliding the filter over an input layer we simultaneously slide it's kernels over all the channels as well but, using different kernels for each layer.\n",
    "\n",
    "![multichannel2](./images/multichannel2.gif)\n",
    "\n",
    "NOTE: The kernels dont have equal weights,some kernels have more weight compared to others. example a red kernel channel with stronger weights than others, responds more to differences in the red channel features than the others.\n",
    "\n",
    "We obtain a single output channel by summing the weights of the kernels in each channel. A biase if the added to obtain the final value, each filter has one biase attached to it the a non-linearlity is applied to it(eg Relu). This process is repeated for all other filters and the results of the filters gets concatenated together to get the final output, with depth(channels) of the final output being the total number of filters used.\n",
    "\n",
    "NOTE: The biase of the filter is broadcasted to each element of the output matrix before being passed through a nonlinearlity function.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "### \\begin{equation}\n",
    "z^{[i]} = w^{[i]}a^{[i]}+b^{[i]}\\\\\n",
    "a^{[o]} = g(z^{[i]})\n",
    "\\end{equation}\n",
    "\n",
    "NOTE: apply **BODMAX**\n",
    "\n",
    "**Where**\n",
    "\n",
    "$ w^{[i]}  = filters $\n",
    "\n",
    "$ a^{[i]} = input layer $\n",
    "\n",
    "$ b^{[i]} = bias $\n",
    "\n",
    "$ a^{[o]} = output of filters $\n",
    "\n",
    "$ g() = non-linearlity function $\n",
    "\n",
    "\n",
    "![multichannel](./images/multichannel.gif)\n",
    "\n",
    "## POOLING\n",
    "\n",
    "### MAX POOLING\n",
    "\n",
    "After we pass the output layer of the convolution throw a non-linearlity function, there is another layer called the max pooling layer, this layer just reduces the dimensionality of the image by reducing the number of pixels in the output from the previous layer.\n",
    "\n",
    "Max pooling layer is added after a convolution layer. In a max pooling layer we define a n X n filter and the stride, we take the maximum value of the pixels covered by the size of the max pooling layer.\n",
    "\n",
    "![maxpool1](./images/maxpool1.gif)\n",
    "\n",
    "### Why max pooling\n",
    "\n",
    "1. Reduce computational load\n",
    "\n",
    "Reducing the dimension of the image to be used in the fully connected layer means we have less parameters hence less computation.\n",
    "\n",
    "2. Reduces overfitting\n",
    "\n",
    "The intuittion behind max pooling is that the pixels with the highest values are the ones with the most prominent features so we wan to pass then on tho the next conv layer or to the fully connected layers. This helpt the model to learn the most prominent features hence avoid overfitting.\n",
    "\n",
    "### AVERAGE POOLING\n",
    "\n",
    "This is another form of pooling in which we take the average of all the pixel values in the area covered by our pooling filter rather than the max of them.\n",
    "\n",
    "![](./images/average_pooling.gif)\n",
    "\n",
    "The result of the max pooling or average pooling is pass on to the next conv net or the fully connected layers.\n",
    "\n",
    "## FULLY CONNECTED LAYERS\n",
    "\n",
    "After passing through all the conv layers and pooling layers its time to pass the final output image to a fully connected NN to make predictions but, how does this happen.\n",
    "\n",
    "The way this works it that as we go deeper into our conv layers the height and the width of the image decreases and the total number of channels increase, the increase in depth is directly proportional to the total number of layers.\n",
    "\n",
    "We then flatten the final resulting image of convolution, relu and pooling layers into a single column mulitple row array(vector) then we feed it to a fully connected layer with softmax.\n",
    "\n",
    "![](./images/full_pic3.gif)\n",
    "\n",
    "![](./images/full_pic2.jpeg)\n",
    "\n",
    "![](./images/full_pic4.png)\n",
    "\n",
    "\n",
    "## CALCULATING TOTAL NUMBER OF PARAMETERS\n",
    "\n",
    "Input layers and pooling layers don't have any parameter, only conv layers and fully connected layers have parameters with the fully connected layers having maximum number of layers\n",
    "\n",
    "Use the formular below to calculate number of parameters in a layer\n",
    "\n",
    "\\begin{equation}\n",
    "params = \\left [ (filter\\_height)^2 * number\\_of\\_channels) + number\\_of\\_channels \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
