{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n",
    "\n",
    "The table below is an example of a confusion matrix.\n",
    "\n",
    "![](./images/confusionmatrix.png)\n",
    "\n",
    "### Basic Terminologies\n",
    "\n",
    "**True positives(TP):** This is the number of time we predicted true(1) and the actual answer was also a true(1). This is the number of data points labelled positive that were actually poistives.\n",
    "\n",
    "**True negatives(TN):** This is the number of times we predicted false(0) and the actual answer was also false(0). This is the number of data points labelled as negative that were actually negative.\n",
    "\n",
    "**False positives(FP)(a Type I error):** This is the number of times we predicted true(1) while the actual answer was false(0).\n",
    "This is the number of data points labelled as positive that were actually negative.\n",
    "\n",
    "**False negatives(FN)(a Type II error):** This is the numbet of times we predicted false(0) while the actual answer was true(1).\n",
    "This is the number of data points labelled as negatives that were actually positives.\n",
    "\n",
    "### Model Evaluation Metrices.\n",
    "\n",
    "These are the rates that help you to make sense of your confusion matrics.\n",
    "\n",
    "1. **Accuracy:** This is the ratio of all correct prediction to the total number of all predictions made by the model. It tells us how often is the model or classifier correct.\n",
    "\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "**Question it answers:** In general, how often is the model prediction correct?\n",
    "\n",
    "2. **Misclassification:** This is the number of times the model made a wrong prediction overall. It tells us overall, how often is the model prediction wrong.\n",
    "\n",
    "$$ Misclassification  = \\frac{FP + FN}{TP + TN + FP + FN} $$\n",
    "\n",
    "$$ Misclassification  = 1 - Accuracy $$\n",
    "\n",
    "**Question it answers:** Generally, how often is the model prediction wrong?\n",
    "\n",
    "3. **Precision:** This is the ration of true positive(TP) to the sum of true positve and false positive. This value tell us how often is the model correct when it predicts a true(1). It's the ability of a classifier to return only relevant instances. Note precision is all about the predictions made\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "**Question it answers:** How often is the model prediction correct when predicting yes(positive)?\n",
    "\n",
    "4. **True Positive Rate, Recall or Sensitivity:** This is the ration of the true positive to the sum of all positive data points in a dataset. It tells us how often does or model(classifier) predict yes when its actually a yes(positive).\n",
    "\n",
    "$$ True \\hspace{0.2cm} Positive \\hspace{0.2cm} \\hspace{0.2cm} Rate = \\frac{TP}{total \\hspace{0.1cm} yes} $$\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "**Question it answers:** Out of all the actual yeses, how often does the model predict yes?\n",
    "\n",
    "5. **False Positive Rate:** This is the ratio of the false positive to total number of negatives in the dataset. It tells us how often does or model(classifier) predict false when its actually a false. Note recall is all about the truth.\n",
    "\n",
    "$$ False \\hspace{0.2cm} Positive \\hspace{0.2cm} \\hspace{0.2cm} Rate = \\frac{FP}{total \\hspace{0.1cm} false} $$\n",
    "\n",
    "$$ False \\hspace{0.2cm} Positive \\hspace{0.2cm} \\hspace{0.2cm} Rate = \\frac{FP}{TN + FP} $$\n",
    "\n",
    "**Question it answers:** How often does our model predict yes, when it's actually no?\n",
    "\n",
    "6. **True Negative Rate or Specificity:** This is the ration of true positive to the total number of negatives in a dataset. It tells us how often our model predicts false when its actually a false.\n",
    "\n",
    "$$ True \\hspace{0.2cm} Negative \\hspace{0.2cm} \\hspace{0.2cm} Rate = \\frac{TN}{total \\hspace{0.1cm} false} $$\n",
    "\n",
    "$$ True \\hspace{0.2cm} Negative \\hspace{0.2cm} \\hspace{0.2cm} Rate = \\frac{TN}{TN + FP} $$\n",
    "\n",
    "**Question it answers:** How often does our model predict no, when it's actually no?\n",
    "\n",
    "7. **Prevelance:** Is how often does yes(positives) occur in our dataset.\n",
    "\n",
    "$$ Prevalance = \\frac{actual \\hspace{0.1cm} yes}{total \\hspace{0.1cm} number \\hspace{0.1cm} of \\hspace{0.1cm} samples } $$\n",
    "\n",
    "8. **F-1 Score:** Is the harmonic mean of the precison and recall.\n",
    "\n",
    "$$ {F_1} \\hspace{0.1cm} score = 2 * \\frac{presicion * recall}{precision + recall} $$\n",
    "\n",
    "\n",
    "9. **Null error rate:** This is how often the model predictions would wrong be if we always predicted the majority class. Example if you data two classes yes:120 and no:60 the null error rate would be 60/180 = 0.3333.  Because if we only predicted the majority class yes, we would be wrong 60 times which is due to the no classes being miss classified. This is a good metric to compare your classifier against. Read more on the [Accuracy Paradon](https://en.wikipedia.org/wiki/Accuracy_paradox) in which models may have a high accuracy but in reality are doing a poor job especially in cases with class imbalance. The best classifier for a particular application will sometimes have a higher error rate than the null error rate.\n",
    "\n",
    "To compare the null error rate to the accuracy of the model simply find the null accuracy which is \\[1 - (null error rate)].\n",
    "\n",
    "If the difference betweent the null accuracy and the model accuracy is low, that may not be a very good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitcb145894da2c40a8833ecd0ec4158689"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
